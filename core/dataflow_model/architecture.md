<!--ts-->
      * [run_pipeline.py](#run_pipelinepy)



<!--te-->

# 

- An experiment in this context is simply code that is parametrized by `Config`s
  - The code contained in an experiment can be anything
  - In this current context, a typical experiment is running a `Dag` through
    a `DagRunner`, where a `Dag` is built from a `Config` and a `DagBuilder`

- There are two flows to run experiments:
  - `run_experiment.py`
  - `run_notebook.py`
  
- These flows
  - run experiments which are represented by a notebook or by a piece of Python
    code, respectively
  - have similar options
  - both have the experiment manager (i.e., `run_experiment.py` and
    `run_notebook.py`) and the actual experiments run in different processes, so that
    experiments can be parallelized and isolated from each other
    
## run_pipeline.py

- TODO(gp): Maybe `run_pipeline` -> `run_experiments`

- Separation of responsibilities
  - `run_pipeline.py` is the experiment manager
    - Run all the experiments
    - Save config for each experiment
    - Parallelize the experiments
    - Implement the retry logic
  - `run_pipeline_stub.py`
    - Runs a single experiment
    - It has the same interface as a notebook
      - Pipeline_builder
      - Config_builder
      - Index
      - Dst_dir

- The invariant is that

- There are two interfaces to materialize configs
  - One on the command line side
  - One on the run_pipeline, notebook side
    - The params to reconstruct the configs are passed through env vars or
      params of the script
```
> run_pipeline.py \
  --dst_dir experiment1 \
  --pipeline ./core/dataflow_model/notebooks/Master_pipeline_runner.py \
  --function "dataflow_lemonade.RH1E.task89_config_builder.build_15min_ar_model_configs()" \
  --num_threads 2
```
  
## `run_notebook.py`
- It is a generic flow for running the same notebook parametrized through several
  `Config`s
- `run_notebook.py`
  - injects the `Config` through env vars into the notebook
  - parallelizes the execution of notebooks for different `Config`s using `joblib`
  - takes care of book-keeping by saving the configs that are run by each notebook
  - handles:
    - retries (`--num_retries`)
    - aborting on error (`--abort_on_error`)
    - skipping running `Config` already run (`--incremental`)
    - etc.
    
- Each notebook:
  - calls `get_config_from_env()` to materialize the config
  - runs top-to-bottom
  - is saved as `ipynb` and published as HTML, if needed

- An invocation is like:
  ```bash
  > run_notebook.py \
    --notebook nlp/notebooks/NLP_RP_pipeline.ipynb \
    --config_builder "nlp.build_configs.build_Task1088_configs()" \
    --dst_dir nlp/test_results \
    --num_threads 2
  ```

- This command line:
  - runs the notebook `nlp/notebooks/NLP_RP_pipeline.ipynb`
  - the notebook is parametrized through the `Config`s generated by the function
    `nlp.build_configs.build_Task1088_configs()`
  - saves the results of each notebook into a separate dir under `nlp/test_results`
    - each dir 
  
### Passing `Config` across processes

- The experiment manager needs to pass the config to execute to each experiment process
  
- The problem is that it's not easy to serialize/deserialize a Config (e.g., a
  `Config` can contain functions and lambda, and it can be thus not pickle-able)

- To work around this issue we pass various information among processes we pass various
  information that allow to build a `Config` in each process
  - The needed information are:
    1) the name of a config builder function that can be executed through an `eval`
       statement
    2) the index of the specific `Config` in the list of available `Config`s to
       select and execute
    3) a destination dir representing the scratch space for the experiment artifacts 

- This information is passed through the "ExperimentRunner" part of a `Config`
  - TODO(gp): Separate `meta` from `run_notebook` since we don't want this
    information to collide.

- We then keep the config in sync between this script and the notebook by executing
  the same code on both sides.

